%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use Frontiers .cls files nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontiers after acceptance.   
%                                              %
%                                                                                                                                                         %
% When submitting your files, remember to upload this *tex file, the pdf generated with it, the *bib file (if bibliography is not within the *tex) and all the figures.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Version 3.4 Generated 2018/06/15 %%%
%%% You will need to have the following packages installed: datetime, fmtcount, etoolbox, fcprefix, which are normally inlcuded in WinEdt. %%%
%%% In http://www.ctan.org/ you can find the packages and how to install them, if necessary. %%%
%%%  NB logo1.jpg is required in the path in order to correctly compile front page header %%%

\documentclass[utf8]{frontiersSCNS} % for Science, Engineering and Humanities and Social Sciences articles

\usepackage{amsmath,amssymb,booktabs,url,hyperref,lineno,listings,microtype,subcaption}

% Automatic formatting of SI units
\usepackage[binary-units]{siunitx}

\usepackage[onehalfspacing]{setspace}

% Required for 'straight' quotes in code listings
\usepackage[T1]{fontenc}

% Visible TODO notes
\newcommand{\todo}[1]{\textbf{\textsc{\textcolor{red}{(TODO: #1)}}}}

\lstset{language=C++,showstringspaces=false,basicstyle=\tiny,upquote=true}

\linenumbers


% Leave a blank line between paragraphs instead of using \\


\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Sample {et~al.}} %use et al only if is more than 1 author
\def\Authors{James C Knight\,$^{1,*}$, Thomas Nowotny\,$^{1}$}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{$^{1}$Centre for Computational Neuroscience and Robotics, School of Engineering and Informatics, University of Sussex, Brighton, United Kingdom }
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{James C Knight}

\def\corrEmail{J.C.Knight@sussex.ac.uk}


\begin{document}
\onecolumn
\firstpage{1}

\title[Running Title]{Article Title} 

\author[\firstAuthorLast ]{\Authors} %This field will be automatically populated
\address{} %This field will be automatically populated
\correspondance{} %This field will be automatically populated

\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}


\maketitle


\begin{abstract}

%%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.
\section{}
Inspired by the distributed nature of memory and computation in the brain, neuromorphic systems are designed to provide a more energy-efficient substrate for emulating spiking neural networks when compared to simulations running on Von Neumann architectures.

While neuromorphic systems may be the ultimate platform for \textit{deploying} spiking neural networks, their very distributed nature makes them unwieldy tools for \textit{developing} SNN models.
Instead development and simulation of SNN models tends to be performed on computers with standard Von Neumann architectures and, once models scale above a certain size, clusters of these machines.
However over the last decade, as well as becoming a common fixture in many workstations, NVIDIA GPU accelerators are now used in \SI{50}{\percent} of the Top 10 super computing sites to increase peak performance.

\todo{argument for using GPUs for SNN}

In this paper we re-implement two large-scale point neuron network models with levels of connectivity and sparseness matching those in the neocortex using our GeNN code generator.
We then verify the correctness of our GPU simulations before comparing the performance and energy usage against published data for traditional CPU-based HPC as well as neuromorphic hardware.
We show that a full-scale model of a cortical column can be simulated faster using a single NVIDIA Tesla V100 accelerator than is possible using 32 nodes of a CPU based cluster or the SpiNNaker neuromorphic system with a significantly lower cost in terms of energy to solution.\todo{specifics}

\tiny
 \keyFont{\section{Keywords:} GPU, high-performance computing, parallel computing, accuracy of simulation, energy to solution, benchmarking, computational neuroscience} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}

\section{Introduction}

\begin{itemize}
    \item Neuromorphic hardware families - limitations, scales etc
    \item HPC - Neuron, NEST etc, recent benchmarks
    \item GPU hardware introduction
    \item Other GPU simulators
\end{itemize}

\section{Material and Methods}
\subsection{GeNN}
\begin{itemize}
    \item GeNN code generator - while CUDA is relatively simple, efficiently parallelising SNN code is non-trivial    
\end{itemize}

GeNN neuron models are defined by writing a class defining the model parameters and snippets of C-like code that describe how it should be simulated.
For example the following \lstinline{LIF} class describes a very simple leaky integrate-and-fire neuron with normalised units, solved algebraically:
%
\lstinputlisting[firstline=2,lastline=15]{code_snippets.cc}
%
The \lstinline{DECLARE_MODEL} and \lstinline{IMPLEMENT_MODEL} macros insert boilerplate code used subsequently for defining parameters and initial model states in a type-safe manner.
The \lstinline{SET_SIM_CODE}, \lstinline{SET_THRESHOLD_CONDITION_CODE} and \lstinline{SET_RESET_CODE} macros specify the snippets of code used, respectively, to update the simulation state, check whether a spike should be emitted and to reset the neuron after a spike.
The names of model parameters (constant across the entire population) are specified using the \lstinline{SET_PARAM_NAMES} macro and any `pre-processing' logic to be applied to these is specified with \lstinline{SET_DERIVED_PARAMS} -- in this case converting an exponential decay time constant to a multiplier to be applied every simulation timestep.
Finally, the \lstinline{SET_VARS} macro specifies the names and types of the per-neuron state variables.
These macros provide some `syntactic sugar' but are entirely optional -- users can instead override the underlying virtual functions themselves.
In GeNN synapse models are defined using very similar classes with the option to define code snippets for time-driven and event-driven updates.
Event-driven updates can be triggered by pre or postsynaptic spikes as well as by custom events for example the pre or postsynaptic neuron's membrane voltages crossing a threshold.
Once the required models have been defined, the parameters and initial state variables values can be set and \textit{populations} of neurons can be added to a network:
%
\lstinputlisting[firstline=17,lastline=20]{code_snippets.cc}
%
This listing also illustrate how the approach used for defining models can also be used to define how variables are initialised.
The membrane voltage \lstinline{V} of our \num{1000} LIF neurons is sampled from the uniform distribution using one of GeNN's built in \textit{variable initialisation snippets}.
These are definited in a similar manner to the neuron and synapse models previously described allowing network initialisation to be parallelised uisng the same strategies used for simulating models on the GPU to be applied to initialising their parameters.
This approach is advantageous as it both removes the need to transfer model state from the CPU to the GPU and allows the GPU to be used to accelerate potentially costly initialisation such as sampling random numbers.
%While the PCI express bus typically used to connect the GPU to the host system is relatively fast (up to \SI{16}{\giga\byte\per\second}), initialising millions of synaptic weights on the CPU and then uploading them at the start of the simulation can still considerably slow down the time it takes to iterate the model.
\todo{Benchmark demonstrating speedup from presentation?}

Once network models have been defined using the C++ interface described in the preceding section, GeNN will generate the CUDA code to simulate the network and this can be linked against a simple simulation loop provided by the user:
%
\lstinputlisting[firstline=22,lastline=32]{code_snippets.cc}
%
While this approach allows a lot of flexibility and means that visualisation tools and closed-loop robotics can be tightly coupled to GeNN simulations, when combined with the use of C++ for model definition, this does make using GeNN a somewhat daunting prospect to users more used to Python-based simulations such as Brian~\citep{Stimberg2014} or PyNN~\citep{Davison2008a}.
Therefore GeNN offers
\todo{Brian2GeNN, SpineML, PyNN}
%
\subsection{The cell-type specific cortical microcircuit}
\label{sec:method/microcircuit}
This model of \SI{1}{\milli\metre\cubed} of early-sensory cortex was first developed by \citet{Potjans2012}.
The model consists of around \num{80000} leaky integrate-and-fire neurons, divided into layers 2/3, 4, 5 and 6; and each layer is modelled as an excitatory and an inhibitory population of neurons.
Neurons in each population are connected with population-specific densities derived from an extensive review of the anatomical literature resulting in a total of approximately \num{0.3E9} synapses.
In addition to this synaptic input, each neuron in the network also receives independent Poisson inputs with population-specific rates representing input from adjacent cortical regions.
Beside this basic structure, the connectivity is random with both synaptic strengths and transmission delays being normally distributed.
\todo{Thomas: is it necessary to go into further details of network structure or just refer reader to Potjans2012?}

\begin{align}
    \tau_{m} \frac{dV}{dt} = & (V - V_{rest}) + R_{m}I
    \tau_{syn} \frac{dI{syn}}{dt} = & 
\end{align}

Although these two state variables are coupled, in our GeNN model, the two equations are solved separately so the current going into equation~\ref{eq:lif_neuron} is effectively treated as a piecewise constant.
While, as \citet{Rotter1999} describes, this 

\begin{enumerate}
    \item LIF exp algabraic solution
    \item Roughly how this is simulated in GeNN: thread per neuron, cuRAND for Poisson, coalesced reads of sparse matrix, fire-and-forget atomics to update input
    \item 
\end{enumerate}

\subsection{Balanced random network with spike-timing dependant plasticity}
\label{sec:method/balanced_random}
The type of one-shot, online learning observed in nature is one of the features of biological neural networks that neuromorphic engineers aspire to emulate.
Synaptic plasticity -- the family of biological mechanisms responsible for changing the strength of synaptic connections in response to neural activity -- has been shown to be fundamental to learning\todo{that citation about optogenetic memory-wiping} and is therefore a key area of computational neuroscience research.

However, adding synaptic plasticity to spiking neural network simulations typically increases the computational cost of simulating them significantly. 
\citet{Morrison2007} reported that adding plasticity to their simulations slowed them down by ``a factor of less than 10'' and \citet{Knight2016b} found that, in the \textit{best} case, simple STDP plasticity reduced the performance of the SpiNNaker neuromorphic system by approximately $6\times$.
Perhaps because of the restrictive cost in computing power and time, there are relatively few large-scale network models with synaptic plasticity.
However, \citeauthor{Morrison2007} argue that it is vital to perform experiments on STDP in models with full-scale connectivity as simplifying connectivity tends to result in fewer, stronger incoming synapses per neuron which will inevitably increase the correlation between neurons.
Because STDP is inherently sensitive to correlated neural activity, artefacts due to this increased correlation are likely to be present in the synaptic weights learned using simplified models.
Therefore \citeauthor{Morrison2007} developed a large balanced random network model~\citep{Brunel2000} with \num{90000} excitatory and \num{22500} inhibitory neurons -- the scale necessary to achieve realistic connection probabilities of $\approx0.1$ and \num{10000} incoming connections per neuron~\citep{braitenberg2013cortex}.
Balanced random networks have been shown to reproduce some of the dynamics seen in the neocortex\todo{citations} and, by adding STDP, \citeauthor{Morrison2007} showed that STDP does not disrupt these dynamics.

\begin{align}
    \Delta w & = \
        \begin{cases}
            \lambda w_{0}^{1-\mu} w^{\mu} e^{-\frac{|\Delta t|}{\tau}}\\ & if\, \Delta t>0\\
            -\lambda \alpha w e^{-\frac{|\Delta t|}{\tau}} & if\, \Delta t\leq0
        \end{cases}
\end{align}

\section{Results}
\subsection{Correctness}
The 
\citet{VanAlbada2018} demonstrated that 

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=180mm]{figures/microcircuit_accuracy}
    \end{center}
    \caption{Spiking output of cortical microcircuit model with Poisson input.  
    \textbf{(A)} Raster plot showing spike times (dots) of neurons from each population. 
    The spikes of 5\% of neurons (vertical) are shown.
    \textbf{(B)} Single-neuron firing rates of all neurons.
    \textbf{(C)} CV ISI, a measure of irregularity of all neurons.
    \textbf{(D)} Correlation coefficients between binned spike trains for \num{200} neurons in each population.
    All measures are calculated over the last \SI{9}{\second} of the simulation and histogram bin widths are determined using the Freedman-Diaconis rule.}
    \label{fig:microcircuit_accuracy}
\end{figure}

\subsection{Performance}
\label{sec:results/performance}
To assess the performance of our GPU simulations we chose a variety GPUs (listed in table~\ref{tab:gpu_devices}) covering a range of financial and power budgets.
CUDA abstracts away the  parallelism the application exposes from the amount of hardware parallelism available so we can run a model that uses \num{80000} threads on a GPU with many fewer CUDA cores.
However memory is a harder constraint and, because of this, while all of the GPUs listed in table~\ref{tab:gpu_devices} can run the microcircuit model described in section~\ref{sec:method/microcircuit}, only the 2 Tesla GPUs have enough memory to run the balanced random network model described in section~\ref{sec:method/balanced_random}.

\begin{table}
  \centering
  \begin{tabular}{r S r S S S S}
    \toprule
        {Model}         & {TDP}             & {Architecture}    & {Num.}    & {Memory}              & {Memory}                      & {Max single-precision}\\
                        & {[\si{\watt}]}    &                   & {CUDA}    & {capacity}            & {bandwidth}                   & {performance}\\
                        &                   &                   & {cores}   & {[\si{\giga\byte}]}   & {[\si{\giga\byte\per\second}]}& {[GFLOPS]}\\
    \midrule
        GeForce 1050 Ti & 75                & Pascal            & 768       & 4                     & 112                           & 2100\\
        Jetson TX2      & 15                & Pascal            & 256       & 4\textsuperscript{1}  & 58.4                          & 12\\
        Tesla K40m      & 245               & Kepler            & 2880      & 12                    & 288                           & 4290\\
        Tesla V100      & 250               & Volta             & 5120      & 16                    & 900                           & 14000\\
    \bottomrule
  \end{tabular}

  \caption{GPU devices.
  \textsuperscript{1}~Memory is shared between CPU and GPU.}
  \label{tab:gpu_devices}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=180mm]{figures/microcircuit_performance}
    \end{center}
    \caption{Simulation times of microcircuit model running on various GPU hardware.
    SpiNNaker and fastest HPC simulation times presented by \citet{VanAlbada2018} included for comparison.
    `Overhead' in GPU simulations refers to time spent in simulation loop but not within CUDA kernels.\todo{10s simulations to match other paper}}
    \label{fig:microcircuit_performance}
\end{figure}

\subsection{Power and energy}
While GPU simulations do not face the issues with quantisation that occur when using fixed point arithmetic, the simulations described in this paper all use single-precision floating point rather than the double-precision used by NEST.
Additionally the non-associative nature of floating point addition and multiplication mean that when a large number of paralellel threads \citep{Villa2009}
when, for example, the input from many thousands on synapses into a neuron are being summed across a massively parallel system add

\section{Discussion}

\subsection{Neurorobotics}
afa

\subsection{Interactive simulation}
asfs

\subsection{Further optimisation and GPU architectures}
The results in section~\ref{sec:results/performance} show that, while the entire initialisation of the balanced random network can be effectively offloaded to the GPU, because the synapses connected using the `FixedNumberTotal' rule have to be initialised on the CPU this adds in the order of \SI{20}{\second} to each microcircuit simulation when run on a workstation or HPC node and almost \SI{10}{\minute} when run on the Jetson TX2.

\begin{itemize}
    \item Generated `fixed number total' connections still performed on CPU - especially when using lower-power ARM CPUs - slow. Cannot be trivially parallelised, but after binomial distribution is sampled for each row can fill rows with synapses by sampling from uniform distribution on GPU. HOWEVER results need to be either sorted (tricky, but Flashsort might be suitable) or values could be sampled in order using rank order statistic.
    \item Since
\end{itemize}
Since the end of ``the free lunch'' -- the point at which it became impractical to continue building ever faster and more complex single-core CPUs -- the only way processor designers have been able to extract more performance from the increasing number of transistors offered by newer fabrication technologies was by build multi-core CPUs. \todo{citations}
However parallel CPU programming paradigms such as multithreading are not well-suited to the type of \textit{fine-grained} parallelism required to harness hundreds of cores.\todo{citations}

The SIMT programming model architecture used by NVIDIA GPUs is designed explicitly for fine-grained parallelism and high throughput.
However spiking neural networks are not especially well-suited

\begin{itemize}
    \item HBM3 - further doubling of memory bandwidth
    \item FP16 - half memory bandwidth, double arithmetic throughput
\end{itemize}

\section*{Conflict of Interest Statement}
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}
The Author Contributions section is mandatory for all articles, including articles by sole authors. If an appropriate statement is not provided on submission, a standard one will be inserted during the production process. The Author Contributions statement must describe the contributions of individual authors referred to by their initials and, in doing so, all authors agree to be accountable for the content of the work. Please see  \href{http://home.frontiersin.org/about/author-guidelines#AuthorandContributors}{here} for full authorship criteria.

\section*{Funding}
Details of all funding sources should be provided, including grant numbers if applicable. Please ensure to add all necessary funding information, as after publication this is no longer possible.

\section*{Acknowledgments}
This is a short text to acknowledge the contributions of specific colleagues, institutions, or agencies that aided the efforts of the authors.

\section*{Data Availability Statement}
The datasets [GENERATED/ANALYZED] for this study can be found in the [NAME OF REPOSITORY] [LINK].
% Please see the availability of data guidelines for more information, at https://www.frontiersin.org/about/author-guidelines#AvailabilityofData
%
\bibliographystyle{frontiersinSCNS_ENG_HUMS}
\bibliography{frontiers_genn}

%%% Make sure to upload the bib file along with the tex file and PDF
%%% Please see the test.bib file for some examples of references

\end{document}
