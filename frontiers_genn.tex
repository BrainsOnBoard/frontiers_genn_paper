% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use Frontiers .cls files nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontiers after acceptance.   
%                                              %
%                                                                                                                                                         %
% When submitting your files, remember to upload this *tex file, the pdf generated with it, the *bib file (if bibliography is not within the *tex) and all the figures.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Version 3.4 Generated 2018/06/15 %%%
%%% You will need to have the following packages installed: datetime, fmtcount, etoolbox, fcprefix, which are normally inlcuded in WinEdt. %%%
%%% In http://www.ctan.org/ you can find the packages and how to install them, if necessary. %%%
%%%  NB logo1.jpg is required in the path in order to correctly compile front page header %%%

\documentclass[utf8]{frontiersSCNS} % for Science, Engineering and Humanities and Social Sciences articles

\usepackage{amsmath,amssymb,booktabs,url,hyperref,lineno,listings,microtype,subcaption}

% Automatic formatting of SI units
\usepackage[binary-units]{siunitx}

\usepackage[onehalfspacing]{setspace}

% Required for 'straight' quotes in code listings
\usepackage[T1]{fontenc}

% Visible TODO notes
\newcommand{\todo}[1]{\textbf{\textsc{\textcolor{red}{(TODO: #1)}}}}

\lstset{language=C++,showstringspaces=false,basicstyle=\tiny,upquote=true,identifierstyle=\ttfamily\color{black}}

\linenumbers


% Leave a blank line between paragraphs instead of using \\


\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Knight and Nowotny} %use et al only if is more than 1 author
\def\Authors{James C Knight\,$^{1,*}$, Thomas Nowotny\,$^{1}$}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{$^{1}$Centre for Computational Neuroscience and Robotics, School of Engineering and Informatics, University of Sussex, Brighton, United Kingdom }
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{James C Knight}

\def\corrEmail{J.C.Knight@sussex.ac.uk}


\begin{document}
\onecolumn
\firstpage{1}

\title[GPUs outperform current SNN simulators]{GPUs
  outperform current HPC and neuromorphic solutions in terms
of speed and energy when simulating a highly-connected cortical model} 

\author[\firstAuthorLast ]{\Authors} %This field will be automatically populated
\address{} %This field will be automatically populated
\correspondance{} %This field will be automatically populated

\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}


\maketitle


\begin{abstract}

%%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.
\section{}
While neuromorphic systems may be the ultimate platform for \textit{deploying} spiking neural networks~(SNNs), their distributed nature and optimisation for specific types of models makes them unwieldy tools for \textit{developing} them.
Instead, SNN models tend to be developed and simulated on computers or clusters of computers with standard Von Neumann CPU architectures.
Over the last decade, as well as becoming a common fixture in many workstations, NVIDIA GPU accelerators have entered the High Performance Computing field and are now used in \SI{50}{\percent} of the Top 10 super computing sites worldwide.
In this paper we use our GeNN code generator to re-implement two neo-cortex-inspired, circuit-scale, point neuron network models on GPU hardware.
We verify the correctness of our GPU simulations against prior results obtained with NEST running on traditional HPC hardware and compare the performance with respect to speed and energy consumption against published data from CPU-based HPC and neuromorphic hardware.
A full-scale model of a cortical column can be simulated at speeds approaching $0.5\times$ real-time using a single NVIDIA Tesla V100 accelerator -- faster than is currently possible using a CPU based cluster or the SpiNNaker neuromorphic system.
In addition, we find that, across a range of GPU systems, the energy to solution as well as the energy per synaptic event of the microcircuit simulation is as much as $14\times$ lower than either on SpiNNaker or in CPU-based simulations.
Besides performance in terms of speed and energy consumption of the simulation, efficient initialisation of models is also a crucial concern, particularly in a research context where repeated runs and parameter-space exploration are required. 
Therefore, we also introduce in this paper some of the novel parallel initialisation methods implemented in the latest version of GeNN and demonstrate how they can enable further speed and energy advantages.

\tiny
 \keyFont{\section{Keywords:} GPU, high-performance computing,
   parallel computing, accuracy of simulation, energy to solution,
   benchmarking, computational neuroscience, spiking neural networks} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}

\section{Introduction}

Currently, the most common way to accelerate large-scale spiking neural network (SNN) simulations is to use CPU-based HPC clusters running software simulators such as NEST~\citep{Gewaltig2007} or parallel Neuron~\citep{carnevale2006neuron}.
However, CPU-based systems are not well-suited to exploiting the large amounts of fine-grained parallelism present in spiking neural network simulations.
Furthermore, in order to reduce simulation times, models must be spread across large numbers of compute nodes meaning that performance is ultimately constrained by the latency of the MPI interconnect.

Neuromorphic systems use dedicated hardware, inspired by aspects of the brain, to address the problems of parallelism and efficient spike communication.
The SpiNNaker system~\citep{Furber2014}, developed as part of the Human Brain project (HBP) in Manchester, is a neuromorphic computer consisting of up to a million ARM cores, connected with an interconnect topology optimised for spike-like communication.
The BrainScaleS system developed within HBP at Heidelberg~\citep{Schemmel2017}, uses analog circuit elements rather than digital processors, to emulate the dynamics of point neurons.
Spikes then propagate between these circuit elements through a digital interconnect network.
Other neuromorphic systems based on various combinations of digital and analog hardware include the Loihi chip~\citep{Davies2018} developed by Intel, the TrueNorth chip~\citep{Merolla2014} built by IBM and the Dynapse system~\cite{Qiao2015} developed at University of Zurich.

While neuromorphic systems offer significant theoretical advantages in terms of power efficiency and simulation speed, this often comes at the expense of flexibility.
In systems where physical circuit elements are used to model individual neurons and synapses, the most obvious restriction is that the physical circuits dictate what neuron and synapse models are supported.
Furthermore, in neuromorphic systems of this type, these circuits are instantiated in a fixed ratio (for example 64k synapses to 256 neurons) meaning that Liebig's law dictates that their scalability is limited by the availability of the scarcest of these circuits.
Even fully-programmable systems such as SpiNNaker suffer from this issue as, for example, handling high incoming spike rates consumes a large number of CPU cycles, reducing the number of neurons that can be simulated on each core.
Some of these issues are illustrated in a recent publication by \citet{VanAlbada2018} who investigated the comparative performance of simulations of a micro-column model~\citep{Potjans2012} in NEST-based simulations on an HPC cluster and an implementation on the SpiNNaker neuromorphic system.
This model required smaller simulation timesteps and denser connectivity than SpiNNaker was designed for, meaning that, although SpiNNaker achieved the same accuracy as the HPC system, it had to be run $20\times$ slower than realtime with only a small number of neurons simulated on each core. 
Running the model this slowly meant that the theoretical energy and performance advantages of using the SpiNNaker system -- which had been previously demonstrated using models more specifically tuned to it's characteristics~\citep{Sharp2012,Sharp2014,Knight2016} -- were lost and the model not only ran faster on the HPC system but also consumed less energy.
%This was not only slower than NEST running on HPC which achieved runtimes of $3\times$ slower than realtime, but also meant
%The lowest energy consumption for NEST was achieved with \num{144} parallel threads when it ran at $4.6\times$ slower than realtime. 
%At this speed SpiNNaker and NEST consumed about the same energy of \SI{6}{\micro\joule} per simulated synaptic event.

Besides measuring the performance in terms of simulation speed, \citet{VanAlbada2018} also identified that efficiently \textit{initialising} and loading large-scale models onto neuromorphic systems remains a computational challenge. 
For example, the cortical microcircuit model developed by \citet{Potjans2012} took \SI{10}{\hour} to initialise and load onto SpiNNaker.
This confirms earlier observations~\citep{Diamond2018} that prototype neuromorphic systems are not efficient at accelerating their initialisation: Both SpiNNaker and a previous generation of the BrainScaleS system spent a significant amount of time and energy initialising network models on a host machine. 

These factors suggest that when \textit{developing} SNNs, more flexible accelerators which can accelerate the construction, initialisation and simulation of large-scale SNNs are required.
Field-Programmable Gate Arrays (FPGAs) are devices consisting of a large number of lookup-table based logic blocks, connected using a programmable fabric.
FPGAs have been used to build various `hard-wired' SNN accelerators~\citep{Moore2012,Wang2018}, but \citet{Naylor2013} showed that they can also be used to develop more flexible, programmable accelerators with comparable performance.
However, although systems of this sort could theoretically be used to accelerate the construction and initialisation of SNNs as well as their simulation, FPGAs are not yet commonplace in workstations and their lack of hardware support for floating point arithmetic makes them ill-suited for simulating some common classes of neuron and synapse models. 

\begin{figure}
    \begin{center}
        \includegraphics{figures/gpu}
    \end{center}
    \caption{Simplified illustration of an example GPU hardware architecture with 3 streaming multiprocessors.}
    \label{fig:gpu}
\end{figure}

Alternatively, GPU architectures are designed for high throughput applications with large amounts of fine-grained parallelism.
They swap the large coherent caches, relied upon by modern CPU architectures to improve performance, for large numbers of floating point arithmetic units connected to high-bandwidth external memory. 
Programmable GPUs were originally developed to accelerate the rendering of 3D graphics where each pixel typically has the same independent computation applied to it -- for example to calculate its illumination.
However GPU acceleration has subsequently proven to be a good fit for accelerating many applications including the training of deep learning systems and, as such, GPUs are used extensively in modern AI systems. 
The application of GPU acceleration to SNN simulations is also promising and there are a number of active SNN simulator projects which target GPUs. 
CARLsim~\citep{Chou2018} is a C++ based simulator using NVIDIA CUDA (Compute Unified Device Architecture) but, as CARLsim is not based on code generation, it is difficult for users without CUDA expertise to add new neuron and synapse models.
EDLUT~\citep{Garrido2011} was initially an event-driven CPU based simulator for SNNs, but has evolved into a hybrid CPU/GPU system with support for both time and event-driven models.
ANNarchy~\citep{Vitay2015} is a code generation based simulator which translates Python model descriptions into multi-core CPU or GPU code with a focus on hybrid rate- and spiking models.
Other simulators that have seen less development in the last 2-4 years include NCS6~\citep{Hoang2013}, Myriad~\citep{Rittner2016}, and NeMo~\citep{Fidjeland2009} (see \citet{Brette2012} for a review).
GeNN \citep{Yavuz2016} is a code-generation library aimed at facilitating accelerated SNN simulations on GPU hardware.
It has been designed to strike a balance between flexibility -- allowing users to define their own model neurons and synapses -- and efficiency in generating optimised CUDA code for the less obviously parallelisable phases of parallel SNN simulations such as spike propagation.

In this paper we introduce novel methods for parallel initialisation of SNNs in the GeNN simulator and investigate the performance of a GPU based simulation in GeNN of the micro-column network model \cite{Potjans2012,VanAlbada2018} as well as a model using STDP in a highly connected network~\citep{Morrison2007}.
We then compare to other recent benchmarks of the models \citep{VanAlbada2018} and critically discuss the current state of the art for SNN simulations.

\section{Material and Methods}
\label{sec:method}
\subsection{GPU architectures}
\label{sec:method/gpu}
In this section we will briefly discuss GPU hardware architectures and the Single Instruction Multiple Thread~(SIMT) paradigm typically used to program them.
Because, in this paper we use NVIDIA hardware and because each manufacturer (confusingly) use their own terminology, we will refer to concepts using NVIDIA's terminology.
However, GPUs built by other manufacturers are relatively similar so, for example, a `Stream Processor' on an AMD GPU is equivalent to a `CUDA core' on an NVIDIA GPU.
Similarily, we will talk about SIMT programming in the context of CUDA because this is what GeNN is implemented in, but OpenCL is conceptually similar.

Figure~\ref{fig:gpu} shows a simplified diagram of the hardware architecture of a typical GPU.
As discussed in the introduction, GPUs are designed primarily for high throughput computation and therefore the majority of their die area is given over to ALUs known as \textit{CUDA cores}.
Depending on the particular GPU, different CUDA cores might be dedicated to integer, single or double-precision floating point operations.
While each CUDA core is independent, they have no capacity for directly executing instructions.
Instead they are contained within \textit{Streaming multiprocessors}~(SMs) which schedule sequences of Single Instruction Multiple Data~(SIMD) instructions known as \textit{warps} to run on the CUDA cores using a piece of hardware known as a \textit{warp scheduler}.
The context associated with each active warp is stored in a large register file (\SI{64}{\kilo\byte} on the Volta architecture) allowing the warp scheduler to very rapidly switch between active warps while they wait for data to be delivered from external memory or for CUDA cores to become available.

In all recent GPU architectures, SMs access the GPU's DRAM through an L2 cache.
While modern CPUs typically have \SI{64}{\bit} memory interfaces, modern GPUs have much wider memory interfaces (\SI{4096}{\bit} on the Volta architecture).
In order to use these wide memory interfaces efficiently, GPU memory controllers aim to combine DRAM accesses made to adjacent memory addresses by the SMs into single transactions -- a processed known as \textit{coalescing}.
Within each SM there is also a small amount (\SI{128}{\kilo\byte} on the Volta architectures) of much faster local memory which can typically be partitioned by the programmer into software-controlled cache known as \textit{shared memory} and read-only hardware controlled L1 cache.

Efficiently programming SIMD architectures often involves manually inserting intrinisics into serial code to process data in parallel.
However, not only is this difficult but, if the underlying architecture and thus the intrinsics which drive it change, applications need to be re-written.
NVIDIA CUDA solves this problem by instead presenting the programmer with a SIMT programming model where programmers write serial code to be executed in parallel across many virtual \textit{threads}.
Threads are grouped into \textit{thread blocks} which are scheduled so that they can share data via the shared memory and the thread blocks are grouped into \textit{grids} which represent all the threads required to solve the entire problem.
The CUDA compiler and GPU hardware take care of converting this representation into warps and scheduling them appropriately as well as enabling and disabling SIMD lanes within each warp when conditional control flow requires it.
For example, adding two vectors \lstinline{x} and \lstinline{y} of length \lstinline{n} could be implemented as follows using CUDA:
\begin{lstlisting}
__global__ void addVec(int n, const float *x, float *y)
{
  const int i = (blockIdx.x * blockDim.x) + threadIdx.x;
  if (i < n) {
    y[i] += x[i];
  }
}
\end{lstlisting}
where, aside from the \lstinline{__global__} function decorator which instructs the compiler to hand this function off to CUDA and the \lstinline{blockIdx}, \lstinline{blockDim} and \lstinline{threadIdx} variables which allow the position of the current thread within the block and grid to be queried, the code is very similar to standard serial C code.

\subsection{GeNN}
\label{sec:method/genn}
As described by \citet{Yavuz2016}, GeNN is a code-generation based system that generates model- and platform-optimised CUDA code for GPU accelerated SNN simulations.
GeNN neuron models are defined by writing a C++ class which defines the model parameters and snippets of C-like code that describe how it should be simulated.
For example the following \lstinline{LIF} class describes a leaky integrate-and-fire neuron with normalised units, solved algebraically:
%
\begin{lstlisting}
class LIF:public NeuronModels::Base
{
public:
  DECLARE_MODEL(LIF,1,1);
  SET_SIM_CODE("$(V)=($(Isyn)*$(TauM)*(1.0-$(ExpTC)))+($(ExpTC)*$(V));\n");
  SET_THRESHOLD_CONDITION_CODE("$(V)>=1.0");
  SET_RESET_CODE("$(V)=0.0;");
  SET_PARAM_NAMES({"TauM"});
  SET_DERIVED_PARAMS({
      {"ExpTC",[](const vector<double> &pars,double dt)
               {return exp(-dt/pars[0]);}}});
  SET_VARS({{"V","scalar"}});
};
IMPLEMENT_MODEL(LIF);
\end{lstlisting}
%
The \lstinline{DECLARE_MODEL} and \lstinline{IMPLEMENT_MODEL} macros insert boilerplate code used subsequently for defining parameters and initial model states in a type-safe manner.
The \lstinline{SET_SIM_CODE}, \lstinline{SET_THRESHOLD_CONDITION_CODE} and \lstinline{SET_RESET_CODE} macros specify the snippets of code used, respectively, to update the simulation state, check whether a spike should be emitted and to reset the neuron after a spike.
The names of model parameters (constant across the entire population) are specified using the \lstinline{SET_PARAM_NAMES} macro and any `pre-processing' logic to be applied to these is specified with \lstinline{SET_DERIVED_PARAMS} -- in this case converting an exponential decay time constant to a multiplier to be applied every simulation timestep.
Finally, the \lstinline{SET_VARS} macro specifies the names and types of the per-neuron state variables.
These macros provide some `syntactic sugar' but are entirely optional -- users can instead override the underlying virtual functions themselves.
In GeNN, synapse models are defined using very similar classes with the option to define code snippets for time-driven and event-driven updates.
Event-driven updates can be triggered by pre or postsynaptic spikes as well as by custom events, for example the pre or postsynaptic neuron's membrane voltages crossing a threshold.
Once the required models have been defined, the values of parameters and initial state variables can be set and \textit{populations} of neurons can be added to a network:
%
\begin{lstlisting}
InitVarSnippet::Uniform::ParamValues vDist(0.0,1.0);
LIF::ParamValues params(20.0);
LIF::VarValues initState(initVar<InitVarSnippet::Uniform>(vDist));
network.addNeuronPopulation<LIF>("pop",1000,params,initState);
\end{lstlisting}
%
This listing also illustrates how, in the latest version of GeNN, the approach used for defining models can also be used to configure how variables are initialised.
In the listing the membrane voltage \lstinline{V} of our \num{1000} LIF neurons is sampled from the uniform distribution using one of GeNN's built in \textit{variable initialisation snippets}.
These are definied in a similar manner to the neuron model presented earlier in this section and, by using this mechanism, GeNN can offload network initialisation to the GPU using the same parallelisation strategies it employs for simulating models.
This approach is advantageous as it removes the need to transfer the model state from the CPU to the GPU and allows the GPU to be used to accelerate potentially costly initialisation operations such as sampling random numbers.

Once network models have been defined using the C++ interface, GeNN will generate a \textit{neuron} CUDA kernel for updating the neuronal state, a \textit{synapse} kernel for simulating the propagation of spike through synaptic connections and, for models with synaptic plasticity, a \textit{postsynaptic learning} kernel. 
GeNN also generates functions for allocating memory (\lstinline{allocateMem}), launching the initialisation kernel (\lstinline{initialize}) and launching each simulation kernel required to advance the simulation state~(\lstinline{stepTimeGPU}).
The generated code can then be linked against a simulation loop provided by the user:
%
\begin{lstlisting}
#include "model_CODE/definitions.h"

int main()
{
  allocateMem();
  initialize();
  while(t < 100.0f) {
     stepTimeGPU();
  }
  return 0;
}
\end{lstlisting}
%
While this approach allows a lot of flexibility and means that visualisation tools and closed-loop robotics can be tightly coupled to GeNN simulations, when combined with the use of C++ for model definition, this does make using GeNN a somewhat daunting prospect for users more used to Python-based simulators such as Brian~\citep{Stimberg2014} or PyNN~\citep{Davison2008a} or graphical tools such as SpineCreator~\citep{Cope2017}.
For these users, GeNN can be used as a backend for other simulators.
Brian2GeNN~\citep{Stimberg2018} allows models defined in Brian 2 to be translated, using code generation, into a valid GeNN simulation. 
Using Brian 2's backend device design, using GeNN through Brian2GeNN is as simple as issuing the command \lstinline[language=python]{set_device("brian2genn")} within a standard Brian 2 script. 
A similar interface exist for SpineCreator and an interface to PyNN~\citep{Davison2008a} is currently under development.
 
\subsection{Cortical microcircuit model}
\label{sec:method/microcircuit}

\begin{figure}
    \begin{center}
        \includegraphics[width=180mm]{figures/potjans_circuit_v2}
    \end{center}
    \caption{Illustration of the microcircuit model.
    Blue triangles represent excitatory populations, red circles represent inhibitory populations and the numbers beneath each symbol shows the number of neurons in each population.
    Connection probabilities are shown in small bold numbers at the appropriate point in the connection matrix.
    All excitatory synaptic weights are normally distributed with a mean of \SI{0.0878}{\nano\ampere} (unless otherwise indicated in green) and a standard deviation of \SI{0.00878}{\nano\ampere}.
    All inhibitory synaptic weights are normally distributed with a mean of \SI{0.3512}{\nano\ampere} and a standard deviation of \SI{0.03512}{\nano\ampere}.}
    \label{fig:potjans_circuit}
\end{figure}

This model of \SI{1}{\milli\metre\cubed} of early-sensory cortex was developed by \citet{Potjans2012} and consists of around \num{80000} neurons, divided into layers 2/3, 4, 5 and 6.
Each layer is modelled by an excitatory and an inhibitory neuron population as shown in figure~\ref{fig:potjans_circuit}.
Neurons in each population are connected randomly with population-specific densities derived from an extensive review of the anatomical literature resulting in a total of approximately \num{0.3E9} synapses.
Beside this structured connectivity, all synaptic strengths and transmission delays are normally distributed.
The membrane voltage ($V_{j}$) of each neuron is modelled as a leaky integrate-and-fire~(LIF) unit:
%
\begin{align}
    \tau_{m} \frac{dV_{j}}{dt} = & (V_{j} - V_{rest}) + R_{m} I_{{in}_{j}} \label{eq:lif_neuron}\\
\end{align}
%
where $\tau_{m}$ and $R_{m}$ represent the time constant and resistance of the neuron's cell membrane, $V_{rest}$ defines the membrane voltage to which the neuron returns if it receives no synaptic input and $I_{{in}_{j}}$ represents the input current to the neuron.
When the membrane voltage crosses a threshold~($V_{thresh}$) a spike is emitted, the membrane voltage is reset back to $V_{rest}$ and a countdown timer is started which, while running, disables the integration of further input thus providing a simulated refractory period.
Incoming spikes induce an exponentially-shaped input current in $I_{{in}_{j}}$:
%
\begin{align}
    \tau_{syn} \frac{dI_{{in}_{j}}}{dt} = & -I_{{in}_{j}} + I_{p_{j}} + \sum_{i=0}^{n} w_{ij} \sum_{t_{i}^{f}}  \delta(t - t_{i}^{f})\label{eq:exp_neuron_input_current}
\end{align}
%
where $\tau_{syn}$ represents the time constant with which any spikes (modelled as Dirac delta functions $\delta$) from $n$ presynaptic input neurons occuring at time $t$ are integrated.
In addition to their synaptic input, each neuron in the network also receives an independent Poisson input current $I_{p_{j}}$ which represents input from adjacent cortical regions.
Finally, $w_{ij}$ represents the peak synaptic input current of the synapse between the presynaptic neuron $i$ and the postsynaptic neuron $j$.
For a full description of the model please refer to \citeauthor{Potjans2012}.
In the remainder of this section we will concentrate on describing the strategies used to parallelise the initialisation and subsequent simulation of this network.

Although the equations describing the neuron dynamics (equations~\ref{eq:lif_neuron}~and~\ref{eq:exp_neuron_input_current}) are coupled, in our GeNN model, the continuous terms of the two equations are solved separately so that the synaptic input current $I_{{in}_{j}}$ going into equation~\ref{eq:lif_neuron} is effectively treated as a constant during each simulation timestep.
As \citet{Rotter1999} explain, this approach leads to a delay of one simulation timestep compared to the exact solution.
However, by separating the solving of these equations, different types of input synapse with different dynamics can trivially be supported.
For example, while a single exponential may be a good approximation of some inhibitory synapses, for other types of synapse the rise time of the post synaptic potential may be vital~\citep{VanVreeswijk1994}.                                                                                                      
Additionally, from a software engineering point-of-view, separating solving these equations improves the encapsulation of neurons and synapses.

Simulating a homogeneous \textit{population} of neurons is an ideal task for a SIMD or SIMT device such as a GPU: the neurons do not communicate with each other and, aside from the relatively rare times that they spike, each neuron will be simulated using exactly the same code path.
Therefore, neural simulation can be trivially parallelised by simulating each neuron on a single thread that fetches the neuron's state variables from global memory into registers at the start of each timestep, advances the simulation state and writes back the state variables.
The Poisson input current~($I_{p_{j}}$) is calculated by generating a Poisson deviate (using the technique described by \citet[p504]{DevroyeLuc2013}) every simulation timestep and multiplying this by a population-specific weight.
As long as the state variables are laid out correctly in memory, the required memory operations can be coalesced so that a \SI{4}{\byte} state variable can be read for \num{32} neurons in a single \SI{128}{\byte} transaction -- the most efficient way to access the global memory.
%This same parallelism strategy is equally valid for neuron models whose dynamics can only be solved numerically -- the advancing of simulation state simply has to be implemented as for example a Runge-Kutta

\begin{figure}
    \begin{center}
        \includegraphics{figures/ragged_matrix}
    \end{center}
    \caption{GPU parallelisation of sparse synaptic matrix processing across two thread blocks each with \num{4} threads.
    $i_{0},\ldots,i_{3}$ contain the indices of presynaptic spikes.
    $l_{i_{0}},\ldots,l_{i_{3}}$ contain the lengths of the corresponding matrix rows.
    $j$ contains the indices of the postsynaptic target neurons.
    Snaking lines indicate CUDA threads.
    Hatching indicates padding entries.}
    \label{fig:ragged_matrix}
\end{figure}

Simulating the sparsely connected synapses between two populations of neurons is, at first glance, less suitable for GPU parallelism.
However, on modern GPU hardware, this can also be implemented in an efficient manner using the data structures shown in figure~\ref{fig:ragged_matrix}.
This structure consists of multiple 2D arrays with rows representing the synapses coming from individual presynaptic neurons and with enough columns to contain the largest number of postsynaptic targets any presynaptic neuron connects to.
One of these 2D arrays contains the indices of the postsynaptic neurons ($j$) and additional arrays are allocated for any individual synaptic state variables such as the synaptic weight~($w_{ij}$) or dendritic delay~($d_{ij}$).
Each block of $N_{block}$ CUDA threads (in figure~\ref{fig:ragged_matrix} $N_{block}=4$) is responsible for processing $N_{block}$ columns of the matrix.
Processing begins by using the $N_{block}$ threads to fetch the indices of $N_{block}$ presynaptic spikes ($i_{0},\ldots,i_{N_{block} - 1}$) and the lengths of the corresponding rows of the matrix ($l_{i_{0}},\ldots,l_{i_{N_{block} - 1}}$) into shared memory (so that these will be accessable to all threads in the block during the next phase).
Threads are then synchronised and loop through the $N_{block}$ rows with each thread processing the synapse in their column.
In the case of the simple static synapses described by equation~\ref{eq:exp_neuron_input_current}, this processing simply consists of reading the index of the postsynaptic target neuron along with the weight $w_{ij}$ and delay $d_{ij}$ associated with the connection and using an atomic add operation to apply the weight to the correct address in the dendritic delay ring-buffer.
This process is repeated until all incoming spikes are processed.
While this parallelism strategy may seem counter-intuitive, it typically performs much better than the naïve approach of using one thread per incoming spike as it not only exposes much more parallelism, but also results in perfectly coalesced memory read operations.
For example, in a simulation with a \SI{0.1}{\milli\second} timestep, a population of \num{10000} neurons firing at an average rate of \SI{10}{\hertz} will only, on average, emit \num{10} spikes in a single timestep.
However, if this population is connected to another population of same size with a \SI{10}{\percent} connection probability, the connection matrix will have over \num{1000} columns resulting in 2 orders of magnitude more parallelism being exposed.
Using the data structures described in this section, GeNN simulations of this model require \SI{3.1}{\giga\byte} of device memory.

An additional advantage of the data structure shown in figure~\ref{fig:ragged_matrix} is that, as long as we know the \textit{maximum} length of any row, memory can be allocated by the host without having to perform any further calculation, meaning that the connectivity itself can be initialised on the GPU.
In this model the density of the synaptic connections between a pair of neuronal populations is specified in terms of a total number of random synapses~($N_{\text{syn}}$) (a \lstinline{FixedNumberTotal} connector in PyNN).
The maximum row length when connecting a presynaptic population with $N_{\text{pre}}$ neurons to a postsynaptic population with $N_{\text{post}}$ neurons using this connectivity can be obtained by evaluating the inverse cumulative distribution function~(CDF) of $\text{Binom}[N_{\text{syn}}, \frac{N_{\text{post}}}{N_{\text{post}} * N_{\text{pre}}}]$ at a suitably high probability (we use $P=0.9999^\frac{1}{N_{\text{pre}}}$).
Once memory is allocated for the data structure, the first stage in initialising the connectivity is to determine how many of the total synapses $N_{\text{syn}}$ end up in each row by sampling from the multinomial distribution $\text{Mult}[N_{\text{pre}} * N_{\text{post}}, \{P_{row}, P_{row}, \ldots, P_{row}\}]$ where $P_{row} = \frac{N_{\text{post}}}{N_{\text{syn}}}$.
This operation cannot be efficiently parallelised so must be performed on the host but, once the length of each row is determined, the postsynaptic targets of the synapses can be initialised in parallel by sampling from the discrete uniform distribution $\text{Unif}[0, N_{\text{post}}]$ using $N_{\text{pre}}$ CUDA threads.
While this works mathematically, in order to improve the locality of memory accesses, synapses should be sorted into ascending order.
This would be trivial to implement in CPU code but, without enough shared memory for each CUDA thread to store a copy of its corresponding row, an in-place sort in global memory would be very slow.
It would be possible to use a more complex parallel sorting algorithm such as that proposed by \citet{Awan2016} but, as GPU architectures typically have very high floating point maths throughput, we instead take an alternative approach.
Rather than sampling directly from $\text{Unif}[0, N_{\text{post}}]$ we sample from its 1st order statistic $\text{Beta}[1, N_{\text{post}}]$ -- essentially the next smallest value.
In the general case the Beta distribution cannot be sampled from in constant time.
However, if $X \sim \text{Beta}[1, N_{\text{post}}]$, $1 - X \sim \text{Beta}[N_{\text{post}}, 1]$ and therefore $-ln(1 - X) \sim \text{Exponential}[N_{\text{post}}]$ -- a much simpler problem as the exponential distribution can be sampled in constant time using the inversion method~\citep[p29]{DevroyeLuc2013}.


\subsection{Balanced random network with spike-timing dependent plasticity}
\label{sec:method/balanced_random}
The type of  online learning observed in nature is one of the features of biological neural networks that neuromorphic engineers aspire to emulate.
Synaptic plasticity -- the family of mechanisms responsible for changing the strength of synaptic connections in response to neural activity -- has been shown to be fundamental to learning~\citep{Nabavi2014} and is therefore a key area of computational neuroscience research.
Spike Timing Dependent Plasticity~(STDP)~\citep{Markram1997,Bi1998} is a popular theory which postulates that these changes are driven by the difference in timing between presynaptic spikes arriving at a synapse and the times at which the postsynaptic neuron itself spikes.
In excitatory cortical~\citep{Markram1997} and Hippocampal~\citep{Bi1998} neurons, synapses at which a presynaptic spike is closely followed by a postsynaptic spike are strengthened, whereas those at which a postsynaptic spike precedes a presynaptic spike are weakened.
The intuition behind this relationship is that synapses that are implicated in the firing of the postsynaptic neuron are strengthened and those that are irrelevant are weakened.

\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/circuit2}
    \end{center}
    \caption{Illustration of the balanced random network model.
    Blue triangles represent excitatory populations, red circles represent inhibitory populations and the numbers beneath each symbol shows the number of neurons in each population.
    Connection probabilities are shown in small bold numbers at the appropriate point in the connection matrix.}
    \label{fig:balanced_random_circuit}
\end{figure}

However, adding STDP to spiking neural network simulations typically increases the computational cost of simulating them significantly. 
\citet{Morrison2007} reported that adding plasticity to their simulations slowed them down by ``a factor of less than 10'' and \citet{Knight2016b} found that, in the \textbf{best} case, simple STDP plasticity reduced the performance of the SpiNNaker neuromorphic system by approximately $6\times$.
Furthermore, the dynamics of neural systems with plasticity operating on biologically-plausible time scales take several orders of magnitude more time to stabilise meaning that longer simulations are required.
%Perhaps because of these issues, there are relatively few large-scale network models with synaptic plasticity.
However, \citeauthor{Morrison2007} argue that it is vital to perform experiments on STDP in models with full-scale connectivity, as simplifying connectivity typically results in fewer, stronger incoming synapses per neuron.
These stronger synaptic inputs will increase the correlation between neurons and, because STDP is inherently sensitive to correlated neural activity, this is likely to cause artefacts in the learned synaptic weights.

Balanced random networks have been shown to reproduce some of the dynamics seen in the neocortex~\citep{Brunel1999,Brunel2000}.
On this basis, \citet{Morrison2007} developed the large balanced random network model shown in figure~\ref{fig:balanced_random_circuit} with \num{90000} excitatory and \num{22500} inhibitory neurons: the scale necessary to achieve a realistic number of incoming connections per neuron of $\approx 10000$~\citep{braitenberg2013cortex} with a biologically plausible connection probability of $\approx 0.1$.
\citeauthor{Morrison2007} showed that adding STDP to this model did not disrupt its dynamics and, as long as a suitable STDP rule is used, the synaptic weights will settle into a stable unimodal distribution.

Similarly to the microcircuit model described in the previous section, this model uses LIF neurons with current inputs.
However, rather than filtering the input current ($I_{{in}_{j}}$) using a single exponential, this model uses slightly more complex \textit{alpha} synapses~\citep{Rall1967} which provide a closer match to the dynamics of biological ion channels:
%
\begin{align}
    \tau_{syn} \frac{dI_{{in}_{j}}}{dt} = & x_{j} - I_{{in}_{j}} \label{eq:alpha_neuron_input_current_1}\\ 
    \tau_{syn} \frac{dx_{j}}{dt} = & -x_{j} + I_{p_{j}} + \sum_{i=0}^{n} w_{ij} \sum_{t_{i}^{f}}  \delta(t - t_{i}^{f}) \label{eq:alpha_neuron_input_current_2}
\end{align}
%
where $x_{j}$ represents a second state variable and all other terms maintain the same meanings they had in equation~\ref{eq:exp_neuron_input_current}.
Nonetheless, equations~\ref{eq:alpha_neuron_input_current_1}~and~\ref{eq:alpha_neuron_input_current_2} have trivial algebraic solutions meaning they can be simulated using the same scheme described in the previous section.

\begin{figure}
    \begin{center}
        \includegraphics{figures/bitmask}
    \end{center}
    \caption{GPU parallelisation of the processing of \num{4} postsynaptic neurons' synaptic input using a bitmask synaptic matrix and one thread blocks with \num{4} threads.
    $i_{0},\ldots,i_{3}$ contain the indices of presynaptic spikes.
    Snaking lines indicate CUDA threads.
    0s and 1s indicate individual bitmask bits.}
    \label{fig:bitmask}
\end{figure}

Even leaving aside synaptic plasticity rules which use postsynaptic membrane voltage~\citep{Brader2007,Clopath2010c} rather than postsynaptic spike times or include `third factors' such as dopamine~\citep{Izhikevich2007}, there are a plethora of different STDP formalisations (see \citet{Morrison2008} for a review).
However, in the model described in this section, \citet{Morrison2007} chose to use a rule that modifies the synaptic weight~($w_{ij}$) between a pre and postsynaptic neuron based on the relative timing of pre~($t_{\text{pre}}$) and postsynaptic~($t_{\text{post}}$) spikes~($\Delta t = t_{\text{post}} - t_{\text{pre}}$):
%
\begin{align}
    \Delta w_{ij} = \
        \begin{cases}
            \lambda w_{0}^{1-\mu} w_{ij}^{\mu} e^{-\frac{|\Delta t|}{\tau}} & if\, \Delta t>0\\
            -\lambda \alpha w_{ij} e^{-\frac{|\Delta t|}{\tau}}             & if\, \Delta t\leq0
        \end{cases}\label{eq:mad_stdp}
\end{align}
%
where $\lambda$ represents the learning rate, $w_{0}$ defines a reference weight and $\mu$ allows the potentiation term to be set as entirely multiplicative~($\mu=1$), entirely additive~($\mu=0$) or somewhere in between.
As discussed by \citet{Morrison2007}, in the model presented in this section, $\mu$ is set to \num{0.4} so as to match the data recorded by \citet{Bi1998}.
Finally $\tau$ defines the time constant of the STDP kernel and $\alpha$ controls the relative strength of potentiation and depression.
\citeauthor{Morrison2007} use this rule with an \textit{all-to-all} spike-pairing scheme meaning that each of the pairs formed by a presynaptic spike and all preceding postsynaptic spikes (and vice-versa) should be considered.
For the full description of this model, please refer to \citeauthor{Morrison2007}.
In the remainder of this section we will concentrate on describing the additional steps required to parallelise models with synaptic plasticity using GeNN.

In order to implement the all-to-all spike pairing required for the model, rather than repeatedly evaluating equation~\ref{eq:mad_stdp}, we calculate updates based on per-neuron \textit{spike traces}~\citep{Song2000, Morrison2007} with the following dynamics:
%
\begin{align}
    \frac{ds_{i}}{dt} & = -\frac{s_{i}}{\tau} + \sum_{t_{i}^{f}}\delta(t - t_{i}^{f}) \label{eq:stdp_trace}
\end{align}
%
The value of these traces can be thought of as representing the sum of the exponential term from equation~\ref{eq:mad_stdp} if it were calculated for every pair or spikes.
Therefore the potentiation~($\Delta w_{ij}^{+}$) induced by the spike pairs formed by a postsynaptic spike occuring at $t_{j}^{f}$ and all preceding presynaptic spikes can be calculated using the following single update:
%
\begin{align}
  \Delta w_{ij}^{+}(t_{j}^{f}) & = \lambda w_{0}^{1-\mu} w_{ij}^{\mu} s_{i}(t_{j}^{f})\label{eq:stdp_update_post_pair}
\end{align}
%
Similarily, the depression~($\Delta w_{ij}^{-}$) induced by the spike pairs formed by a presynaptic spike occurring at $t_{i}^{f}$ and all preceding postsynaptic spikes can be calculated using the following single update:
\begin{align}
    \Delta w_{ij}^{-}(t_{i}^{f}) & = \lambda \alpha w_{ij} s_{j}(t_{i}^{f})\label{eq:stdp_update_pre_pair}
\end{align}
%
In GeNN, if a neuron has fired in the current timestep, its trace variables are updated in the neuron kernel by evaluating equation~\ref{eq:stdp_trace}.
Synaptic depression is calculated by applying equation~\ref{eq:stdp_update_pre_pair} to each synaptic weight processed in the synapse kernel described in the previous section.
Similarly, calculating synaptic potentiation involves applying equation~\ref{eq:stdp_update_post_pair} to each synapse targetting a spiking postsynaptic neuron.
However this is tricky as while the data structure shown in figure~\ref{fig:ragged_matrix} supports efficient \textit{row-wise} access to the synapses associated with a presynaptic neuron, like many sparse matrix data structures, it does not support efficient \textit{column-wise} accesses to the synapses associated with a postsynaptic neuron.
This is a problem faced by all SNN simulators that support STDP.
Some small-scale neuromorphic systems have solved this problem in hardware using custom SRAM memories which allow both column and row-wise accesses~\citep{Seo2011}.
However, custom SRAMs are expensive in terms of silicon area, so many neuromorphic systems avoid the problem entirely by implementing synaptic plasticity rules which use the membrane voltage of the postsynaptic neuron rather than its spike times -- meaning that no updates triggered by postsynaptic spikes are required~\citep{Frenkel2018,Qiao2015}.
Intel's Loihi system~\citep{Davies2018} and the SpiNNaker software developed by \citet{Galluppi2014a} take an alternative approach and defer all STDP updates until the end of a ``learning epoch'' after which time they are processed sequentially row by row.
NEST~\citep{Morrison2007} and the more recent SpiNNaker software~\citep{Knight2016} both buffer postsynaptic spikes until the next presynaptic spike occurs -- allowing weight updates triggered by pre and postsynaptic spikes to be applied in order without having to make any column-wise accesses to the synaptic matrix. 
However, buffering postsynaptic spikes makes access to other postsynaptic variables difficult as they would also need to be buffered for an unbounded length of time until the next presynaptic spike occurs.

Deferring STDP updates ideally requires a dynamic memory structure to store postsynaptic events which, when combined with the need to search through this data structure for events to apply, means that this approach does not appear to be well-suited for GPU implementation.
Furthermore, GeNN aims to support a wide range of synaptic plasticity rules with full access to pre and postsynaptic neuron variables.
%The approach developed by \citet{Galluppi2014a} de
%By setting the duration of each epoch to match the minimum refractory period of neurons in the network, Loihi guarantees that each synapse only needs to process a maximum of one pre and postsynaptic spike during a learning epoch, significantly simplifying the update process.
%While this approach is potentially well-suited to GPU implementation, to ensure correctness when more complex neuron models without a fixed refractory period are used, the learning epoch would have to occur every simulation timestep.
Therefore, GeNN builds an additional column-major sparse matrix using the same data structure shown in figure~\ref{fig:ragged_matrix}, containing indices into the original row-wise arrays containing synaptic weights.
%Synaptic potentiation is then applied using an additional \textit{postsynaptic learning} CUDA kernel which applies equation~\ref{eq:stdp_update_post_pair} to columns of synaptic weights using a similar strategy to the synapse kernel where each CUDA thread is responsible for applying each postsynaptic spike to all synapses in a row of the new data structure.
This has the downside of doubling the memory requirements of connections when STDP is required and, as \citet{Yavuz2016} demonstrated, the resultant non-coalesced accesses to the synaptic matrix reduce performance on lower-end GPUs. 
However, the approaches involving buffering of events and variables described above come with their own challenges in terms of memory management and minimising the divergence of execution between CUDA threads.
Furthermore, the performance reductions due to non-coalesced memory accesses are much less severe on modern GPUs due to the ever-increasing size of their L2 cache.

In the balanced random network model, the synaptic weights of the non-plastic connections are initialised to a constant value so the GeNN code generator can compile these constants directly into the synapse kernels. 
While this results in significant memory savings, it is not enough to fit the model onto GPUs with \SI{12}{\giga\byte} of memory using either of GeNN's standard sparse matrix formats.
We, therefore, use the alternative \textit{bitmask} data structure shown in figure~\ref{fig:bitmask} to store the non-plastic connections on these GPUs.
When using the \textit{bitmask} data structure, the connections between a presynaptic population with $N_{\text{pre}}$ neurons and a postsynaptic population with $N_{\text{post}}$ neurons are stored using a $N_{\text{pre}} \times N_{\text{post}}$ bit bitfield (rounded up to the nearest \SI{32}{\bit} word).
For example, the connections between the excitatory (\num{90000} neurons) and inhibitory populations (\num{22500} neurons) in the balanced random network model can be stored in \SI{241}{\mebi\byte} using a bitmask rather than \SI{867}{\mebi\byte} when using the data structure described in the previous section.
Using the bitmask the total amount of device memory required to simulate this model using GeNN from \SI{11.5}{\giga\byte} to \SI{10.2}{\giga\byte}.
The bitmask data structure is processed using a CUDA thread to accumulate each postsynaptic neuron's input into a register every simulation timestep.
Each of these threads loops through the incoming spikes stored in the shared memory data structure described in the previous section and, if the corresponding bit in the bitmask is set, adds the synaptic weight to the register.

Similarly to the data structure shown in figure~\ref{fig:ragged_matrix}, the amount of memory required to store synapses in the bitmask data structure can be be calculated without any knowledge of the connectivity within, meaning that synapses stored in this format can also be initialised on the GPU.
In this model, the density of the synaptic connections is described using a probability of connection $P$.
Therefore, whether a synapse exists between a pair of pre and postsynaptic neurons can be described using a Bernoulli distribution $\text{Bern}[P_{\text{conn}}]$.
While the Bernoulli distribution can be sampled by repeatedly drawing from the uniform distribution $\text{Unif}[0, 1]$ and comparing each sample to $P$, this is innefficient for sparse connectivity.
Instead we sample from the geometric distribution $\text{Geom}[P_{\text{conn}}]$ which describes how the number of Bernoulli trials required to get a success (i.e. a synapse) is distributed.
The geometric distribution can be sampled in constant time by inverting the cumulative density function~(CDF) of the equivalent continuous distribution (the exponential distribution) to obtain $\frac{log(\text{Unif}[0, 1])}{log(1 - P_{\text{conn}})}$~\citep[p499]{DevroyeLuc2013}.
Using this approach, generating fixed probability connectivity can be performed entirely in parallel by initialising each row of connectivity using an independent CUDA thread.

\begin{figure}
    \begin{center}
        \includegraphics[width=180mm]{figures/microcircuit_accuracy}
    \end{center}
    \caption{Spiking output of cortical microcircuit model with Poisson input.
    All measures are calculated over the last \SI{9}{\second} of the simulation and histogram bin widths are determined using the Freedman-Diaconis rule.\\
    \textbf{(A)} Raster plot showing spike times (dots) of neurons from each population.
    The spikes of 5\% of neurons (vertical) are shown.\\
    \textbf{(B)} Single-neuron firing rates of all neurons.\\
    \textbf{(C)} CV ISI, a measure of irregularity of all neurons.\\
    \textbf{(D)} Correlation coefficients between binned spike trains for \num{200} neurons in each population.}
    \label{fig:microcircuit_accuracy}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/microcircuit_accuracy_kl}
    \end{center}
    \caption{Comparison of per-population distributions of dynamical properties shown in figure~\ref{fig:microcircuit_accuracy}.
    Comparisons calculated using the Kullback-Leibler~(KL) divergence with NEST running in `precise' mode as a reference.
    For comparison, KL divergences for NEST running in `grid-aligned' mode and SpiNNaker are read off figure presented by \citeauthor{VanAlbada2018}.\\
    \textbf{(A)} Single-neuron firing rates.\\
    \textbf{(B)} CV ISI.\\
    \textbf{(C)} Correlation coefficients.}
    \label{fig:microcircuit_accuracy_kl}
\end{figure}


\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/mad_weights}
    \end{center}
    \caption{Histogram of the synaptic weight distribution obtained after \SI{2000}{\second} of simulation.
    The solid red line shows the gaussian distribution with $\mu_{w} = 46.25$ and $\sigma_{w} = 4.07$.}
    \label{fig:mad_weights}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{r S S}
    \toprule
        {Statistic}                                     & {Value reported by}       & {Value obtained from} \\
                                                        & {\citet{Morrison2007}}    & {GeNN simulation} \\
    \midrule
        Mean weight [\si{\pico\ampere}]                 & 45.65                     & 46.25 \\
        Weight standard deviation [\si{\pico\ampere}]   & 3.99                      & 4.07 \\
        Mean spike rate [\si{\hertz}]                   & 8.8                       & 8.8 \\
        Covariance of interspike interval               & 0.88                      & 0.86 \\
        Fano factor                                     & 8.5                       & 8.3 \\
    \bottomrule
  \end{tabular}

  \caption{Comparison of statistics reported by \citet{Morrison2007} with those obtained from our GeNN simulations.}
  \label{tab:mad_stats}
\end{table}

\section{Results}
\subsection{Correctness}
\label{sec:results/correctness}
In this section we will focus on confirming the correctness of our simulations of the microcircuit model~\citep{Potjans2012} described in section~\ref{sec:method/microcircuit} using the methodology described by \citet{VanAlbada2018}.
Additionally we will compare the results of simulations of the balanced random network model described in section~\ref{sec:method/balanced_random} to those reported by \citet{Morrison2007}.

\subsubsection{Cortical microcircuit model}
\citet{VanAlbada2018} performed an in-depth analysis of the correctness of simulations of the microcircuit model -- running both on NEST and on the SpiNNaker neuromorphic system -- using NEST running in `precise' mode as a ground-truth.
In `precise' mode, rather than constraining spike events to simulation time steps, NEST communicates the exact time at which neurons' membrane voltages cross the threshold between the nodes simulating the model~\citep{Hanuschkin2010}.
As \citeauthor{VanAlbada2018} describe, while the randomisation of each neuron's initial membrane voltage should reduce any such effects, the first \SI{1}{\second} of spike data from each \SI{10}{\second} simulation is discarded in order to remove any transients.
We then calculated the average firing rates and the covariance of interspike intervals~(CV ISI) for each neuron in the model over the remaining \SI{9}{\second} of the simulation using the Elephant~\citep{Yegenoglu2018} package.
We also picked \num{200} (this was a trade-off between accuracy and analysis time chosen by \citeauthor{VanAlbada2018}) active neurons from each population, binned their spike trains into \SI{2}{\milli\second} bins (corresponding to the refractory time of the neurons) and calculated the Pearson correlation coefficients matrix between each disjoint pair of neurons.

The same measures were calculated from a NEST simulation run in `precise' mode and histograms of all three measures were produced for both simulations using bins calculated from the NEST data using the Freedman-Diaconis rule~\citep{Freedman1981}.
The histograms were smoothed with Gaussian kernel density estimation performed using the \lstinline{scipy.stats.gaussian_kde} function with bandwidths of \SI{0.3}{\per\second}, \num{0.04} and \num{0.002} for the average firing rates, CV ISI and correlation respectively.

Figure~\ref{fig:microcircuit_accuracy} shows the results of this analysis.
Visually it is clear that the per-population distributions are highly similar and, to quantify this, we calculated the Kullback-Leibler~(KL) divergences using the `precise' NEST data as the reference.
Figure~\ref{fig:microcircuit_accuracy_kl} shows the KL divergences calculated from our GeNN simulation as well as those reported by \citet{VanAlbada2018} for their grid-aligned NEST and SpiNNaker simulations and between two `precise' NEST simulations with different random number generator seeds. 
Similarly to those calculated from the SpiNNaker and grid-aligned NEST simulations, the KL divergences from our GeNN simulation are comparable in size to those caused by changing the random number generator seed.

\subsubsection{Balanced random network}
To assess the correctness of our implementation of the balanced random network model described in section~\ref{sec:method/balanced_random}, we simulated the network for \SI{2000}{\second} of biological time and compared the final weight distribution and the statistics of the last \SI{50}{\second} of spiking activity with those reported by \citet{Morrison2007}.
The calculated statistics are listed in table~\ref{tab:mad_stats} and the final weight distribution is shown in figure~\ref{fig:mad_weights}.
%The final weight distribution has a small right skew, but is otherwise similar to that reported by \citeauthor{Morrison2007}.
To quantify the network dynamics resulting from these synaptic weights, we calculate the mean firing rate and CV ISI of all the excitatory neurons in the network using the Elephant~\citep{Yegenoglu2018} package.
The mean firing rate and CV ISI values listed in table~\ref{tab:mad_stats} suggest that our model had settled into a very similar asynchronous-irregular regime to that reported by \citeauthor{Morrison2007}.
Our model exhibited fast oscillations throughout the simulation and, to quantify the resultant variation in spike rate, we calculated a histogram with \SI{3}{\milli\second} bins from the output spike trains of \num{1000} excitatory neurons.
By dividing the variance of each bin's spike count by its mean we calculated a Fano factor which, again, was very simular to that reported by \citeauthor{Morrison2007}.
As \citet{Pauli2018} thoroughly demonstrate, reproducing results from spiking neural network models on different simulators can be difficult, especially with models of this age where the original code is not publically available.
Therefore we believe that the remaining small differences in results are likely to be due either to numerical differences caused by single-precision floating point and our use of CUDA's approximate exponential and power functions; or to subtle differences in the order of operations between GeNN and NEST.

\subsection{Performance}
\label{sec:results/performance}
To assess the performance of our GPU simulations we chose a selection of GPUs listed in table~\ref{tab:gpu_devices} -- covering a range of financial and power budgets.
CUDA abstracts away the degree of parallelism exposed by the application from the amount of hardware parallelism available so we can run a model that uses \num{80000} threads on a GPU with many fewer CUDA cores.
However, memory is a harder constraint so, while all of the GPUs listed in table~\ref{tab:gpu_devices} can run the microcircuit model described in section~\ref{sec:method/microcircuit}, due to the increased memory requirements of STDP connections, only the two `Tesla' GPUs have enough memory to run the balanced random network model described in section~\ref{sec:method/balanced_random}.

\begin{table}
  \centering
  \begin{tabular}{r S r S S S S}
    \toprule
        {Model}         & {Thermal Design}  & {Architecture}    & {Num.}    & {Memory}              & {Memory}                      & {Max single-precision}\\
                        & {Power (TDP)}     &                   & {CUDA}    & {capacity}            & {bandwidth}                   & {performance}\\
                        & {[\si{\watt}]}    &                   & {cores}   & {[\si{\giga\byte}]}   & {[\si{\giga\byte\per\second}]}& {[GFLOPS]}\\
    \midrule
        GeForce 1050 Ti & 75                & Pascal            & 768       & 4                     & 112                           & 2100\\
        Jetson TX2      & 15                & Pascal            & 256       & 8\textsuperscript{1}  & 58.4                          & 750\\
        Tesla K40c      & 235               & Kepler            & 2880      & 12                    & 288                           & 4290\\
        Tesla V100      & 250               & Volta             & 5120      & 16                    & 900                           & 14000\\
    \bottomrule
  \end{tabular}

  \caption{GPU devices.\\
  \textsuperscript{1}~Memory is shared between CPU and GPU.}
  \label{tab:gpu_devices}
\end{table}

\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/microcircuit_performance}
    \end{center}
    \caption{Simulation times of the microcircuit model running on various GPU hardware for \SI{10}{\second} of biological time.
    SpiNNaker and fastest HPC simulation times (12 nodes) presented by \citet{VanAlbada2018} included for comparison.
    `Overhead' in GPU simulations refers to time spent in simulation loop but not within CUDA kernels.
    The dotted horizontal line indicates realtime performance.}
    \label{fig:microcircuit_performance}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/microcircuit_init_performance}
    \end{center}
    \caption{Initialisation times of the microcircuit model running on various GPU hardware.
    SpiNNaker and fastest HPC simulation times (32 nodes) presented by \citet{VanAlbada2018} included for comparison.}
    \label{fig:microcircuit_init_performance}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/microcircuit_scaling}
    \end{center}
    \caption{Simulation times of the microcircuit model running at different scales on various GPU hardware for \SI{10}{\second} of biological time.
    Downscaling rules described by \citet{VanAlbada2015} are employed to maintain spiking statistics across scales.}
    \label{fig:microcircuit_performance_scaling}
\end{figure}

We measured the performance of both models by querying the \lstinline{std::chrono::high_resolution_clock} around the simulation loop and by using CUDA's own event timing system~\citep[Section~3.2.5.6.2]{NVIDIACorporation2018} to record the time taken by the neuron and synapse simulation kernels as well as the postsynaptic learning kernel in the balanced random network model.

\subsubsection{Cortical microcircuit model}
\label{sec:results/performance/microcircuit}
Figure~\ref{fig:microcircuit_performance} shows the simulation times of the microcircuit model running on each GPU for \SI{10}{\second} of biological time, including the times taken by neuron and synapse simulation kernels.
Compared to the smaller point neuron benchmark presented by \citet{Yavuz2016}, even though each neuron in our model receives up to $10\times$ as many synaptic inputs, the simulation time is more evenly split between the simulation of neurons and synapses.
This is partly because our simulations are running with a smaller \SI{0.1}{\milli\second} timestep meaning that less presynaptic spikes are processed each timestep.
Additionally, in the newer version of GeNN used in this paper, the generation of Poisson noise takes place in the neuron kernel rather than in the separate kernel used by \citeauthor{Yavuz2016}.

In general, as one would expect, the two Tesla GPUs perform best with the newer Tesla V100 system achieving a faster simulation speed than was possible on the CPU-based HPC cluster \citep{VanAlbada2018}.
However even the GeForce 1050ti -- which is a low-end gaming GPU -- can simulate the model faster than the SpiNNaker system.

As discussed in section~\ref{sec:method/genn}, as well as parallelising neuron and synapse simulation code, the latest version of GeNN also parallelises the initialisation of model state variables and connectivity using the GPU.
Figure~\ref{fig:microcircuit_init_performance} shows the initialisation time of the microcircuit simulation when initialisation is performed on the CPU compared to the GPU.
Even on the two Tesla systems which have Intel Xeon CPUs with high single-threaded performance, using the GPU for initialisation, results in a speedup of around $20\times$ and on the Jetson TX2, with its much slower ARM A57 CPU, GPU initialisation is more than $150\times$ faster.

Figure~\ref{fig:microcircuit_init_performance} also includes the initialisation times for SpiNNaker and the fastest HPC configuration presented by \citet{VanAlbada2018}.
The scaling plot for HPC initialisation presented by \citeauthor{VanAlbada2018} confirms the trivially parallelisable nature of network initialisation compared to simulation -- performance continued to increase up to 32 nodes rather than just 12 in the simulation.
However, all three desktop GPU systems still perform network initialisation in a shorter time than the HPC system.
\citet{Diamond2018} concluded that initialisation and loading time was a big problem for neuromorphic systems and SpiNNaker clearly still has issues in this area as initialising and loading the microcircuit network onto the SpiNNaker system takes approximately \SI{10}{\hour}.
This is approximately $50\times$ slower than the Jetson TX2 when only one of its ARM cores is used for initialisation.

To illustrate how the run-time of GPU simulations varies with the scale of the model we also simulated scaled down versions of the microcircuit model on all four GPU devices.
Scaling is performed using the downscaling rules described by \citet{VanAlbada2015} which aim to preserve the spiking statistics of the model.
When simulating the model with a target number of neurons $N_t$, we calculate a scaling factor $K = \frac{N_t}{77169}$.
The mean synaptic weights are scaled down by $\sqrt{K}$.\todo{understand and then explain scaling}
Figure~\ref{fig:microcircuit_performance_scaling} shows the results of these simulations and sugge

\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/stdp_performance}
    \end{center}
    \caption{Simulation times of the balanced random network model running on various GPU hardware for \SI{200}{\second} of biological time.
    `Overhead' in GPU simulations refers to time spent in simulation loop but not within CUDA kernels.
    `Standard' and `Bitmask' refer to the data structure used for representing the model's non-plastic connections -- the standard synaptic matrix data structure described in section~\ref{sec:method/microcircuit} or the bitmask data structure described in section~\ref{sec:method/balanced_random} respectively.
    The dotted horizontal line indicates realtime performance.}
    \label{fig:stdp_performance}
\end{figure}

\subsubsection{Balanced random network}
Figure~\ref{fig:stdp_performance} shows the runtime of simulations of the balanced random network model described in section~\ref{sec:method/balanced_random}.
The Tesla V100 has enough memory~(\SI{16}{\giga\byte}) to represent the model's non-plastic connections using the standard synaptic matrix data structure described in section~\ref{sec:method/microcircuit} as well as the bitmask data structure described in section~\ref{sec:method/balanced_random}.
However, figure~\ref{fig:stdp_performance} shows that this has a negligible impact on the simulation time, suggesting that both data structures are equally efficient for large-scale models.
While \citet{Morrison2007} report that their simulations of this model took \SI{60}{\hour} to run for \SI{1000}{\second} of biological time on their HPC cluster -- which is around $4\times$ slower than our simulations run on the Tesla K40c -- we have not included this in figure~\ref{fig:stdp_performance} as a comparison with decade-old CPU hardware would not be a fair one.


\subsection{Power and energy}
\label{sec:results/power}
\begin{figure}
    \begin{center}
        \includegraphics[width=85mm]{figures/microcircuit_power}
    \end{center}
    \caption{Power consumption during \SI{10}{\second} microcircuit simulation.
    Power was measured using consumer power measurement device with minimum resolution of \SI{0.1}{\watt} at mains socket.\\
    \textbf{(A)} Tesla K40c in a workstation with a Intel Xeon E5-1620 v2 processor running Ubuntu 16.04 LTS.\\
    \textbf{(B)} GeForce 1050Ti in a desktop PC with an Intel Core i5 750 processor running Windows 7.\\
    \textbf{(C)} NVIDIA Jetson TX2 development kit running Ubuntu 16.04 LTS and JetPack 3.2.1 in maximum performance mode.}
    \label{fig:microcircuit_power}
\end{figure}

As well as recording the runtimes of the microcircuit benchmark described in the previous section, we also recorded the power usage of the systems being benchmarked using a consumer power measurement device at the mains socket.
The screen of the power measurement device was recorded using a webcam, optical character recognition was performed using `Seven Segment Optical Character Recognition' developed by \citet{Auerswald2018} and the resultant power measurements were tagged with a time and written to disk.
Figure~\ref{fig:microcircuit_power} shows the power usage over time for simulations of the microcircuit model running for \SI{10}{\second} of biological time on each of the devices listed in table~\ref{tab:gpu_devices} aside from the Tesla V100 to which we do not have local access.

\begin{table}
  \centering
  \begin{tabular}{r S S S}
    \toprule
        {Model}                 & {Energy to solution}      & {Simulation energy}       & {Energy per synaptic event} \\
                                & {[\si{\kilo\watt\hour}]}  & {[\si{\kilo\watt\hour}]}  & {[\si{\micro\joule}]} \\
    \midrule
        GeForce 1050 Ti         & 0.0053                    & 0.0051                    & 2.0 \\
        Jetson TX2              & 0.00080                   & 0.00078                   & 0.30  \\
        Tesla K40c              & 0.0030                    & 0.0028                    & 1.08 \\
        SpiNNaker               & {--}                      & 0.017                     & 5.9 \textsuperscript{1}\\
        NEST (lowest energy)    & {--}                      & 0.012                     & 4.4 \\
    \bottomrule
  \end{tabular}

  \caption{Energy cost of simulations.
  Energy to solution and simulation energy of GPU are calculated using the \lstinline{numpy.trapz} function and the simulation energy is divided by the total number of synaptic events processed to obtain the energy per synaptic event.
  For comparison, simulation energies and energies per synaptic event for SpiNNaker and the NEST simulation with the lowest simulation energy (2 nodes) are read off the figure presented by \citet{VanAlbada2018}.
  \textsuperscript{1}~This energy per synaptic event is calculated after the `idle' power of the SpiNNaker system has been taken into account.}
  \label{tab:energy_measures}
\end{table}

By integrating the power time series using the \lstinline{numpy.trapz} function we calculated the energy to solution for each device as well as the energy per synaptic event -- a common measure for comparing the energy efficiency of neuromorphic systems.
These energy costs are listed in table~\ref{tab:energy_measures} alongside the energy costs presented by \citet{VanAlbada2018} for simulations running on SpiNNaker and a CPU-based cluster.
Whilst we were unable to measure the energy of the Tesla V100 system directly, Tesla GPUs have built in power monitoring which shows that the Tesla V100 drew a maximum of \SI{88}{\watt} compared to \SI{107}{\watt} for the Tesla K40c.
As the workstation containing the Tesla K40c drew \SI{218}{\watt} while simulating the model, compared to an idle power draw of \SI{84}{\watt}, we can estimate that the single CPU core being used by the simulation was drawing \SI{27}{\watt} more than when the system was idle.
Therefore we can estimate that, if a Tesla V100 was attached to the same workstation, the maximum power draw would be reduced to \SI{199}{\watt} suggesting that, based on the reduced simulation time of \SI{22}{\second}, the simulation energy for such a system would be \SI{0.0012}{\kilo\watt\hour} and the energy per synaptic event would be \SI{0.47}{\micro\joule}.

From figure~\ref{fig:microcircuit_power} we can see that even an idling workstation draws on the order of \SI{100}{\watt} and, as \citeauthor{VanAlbada2018} discusses, a single Infiniband switch has a TDP of over \SI{200}{\watt}.
Therefore it is somewhat unsurprising that any accelerator that allows equivalent simulations to be run on fewer nodes would significantly improve energy usage.

\section{Discussion}
\subsection{Suitability of GPU architectures for SNN simulations}
The $3.5\times$ increase in peak performance and the $3.1\times$ increase in memory bandwidth between the Tesla K40c (released in 2013) and the Tesla V100 (released in 2017) listed in table~\ref{tab:gpu_devices} illustrate just how much GPUs have taken advantage of Moore's law scaling.
This scaling is reflected in the runtime of our simulations where the cortical microcircuit model ran approximately twice as fast on the newer Tesla V100 GPU.
However, the simulations of the plastic model ran more than $10\times$ faster on the Tesla V100, suggesting that recent architectural changes have further improved the suitability of GPUs for SNN simulations.
Figure~\ref{fig:stdp_performance} shows that the improved performance of the Tesla V100 is almost entirely due to the reduction of time spend in the postsynaptic learning kernel.
This kernel is where synaptic potentiation is applied using the approach outlined in section~\ref{sec:method/balanced_random} in which an additional column-major sparse matrix structure is used to select weights to update in response to postsynaptic spikes.
We believe that two new features of the Volta architecture~\citep{Nvidia2017} used by the V100 GPU are playing a crucial role in accelerating this kernel.
Firstly, Volta GPUs have \SI{6144}{\kibi\byte} of L2 cache compared to only \SI{1536}{\kibi\byte} in the older Kepler architecture used by the Tesla K40c, which helps overcome the cost of the non-coalesced accesses to synaptic weights.
Additionally, Volta GPUs can now simultaneously execute integer and floating point operations, meaning that the pointer arithmetic required to calculate the indices into the synaptic weight matrix can be performed simultaneously with the actual learning rule update.

In our simulations of both models, we copy all spikes from the GPU to the host computer at the end of each simulation timestep.
Along with the overhead involved in launching CUDA kernels every simulation timestep, the copying of spikes accounts for the majority of the `overhead' shown in figures~\ref{fig:microcircuit_performance}~and~\ref{fig:stdp_performance}.
Furthermore, because the microcircuit model has four times as many neuronal populations as the balanced random network model, copying its spiking output requires more interactions with the GPU driver, resulting in the higher overhead seen in simulations of this model.
The overhead is particularly high on the GeForce 1050ti system which we believer is due to a combination of the slower PCI express bus in this machine (PCIe Gen 2 rather than Gen 3) and issues using CUDA on display devices under Windows.
When simulating the microcircuit at smaller scales this problem is exacerbated so, at the smallest scale shown in figure~\ref{fig:microcircuit_performance_scaling}~(\num{19292} neurons), these overheads account for between $65-90\si{\percent}$ of the simulation time on the discrete GPUs.
However, CUDA allows for memory operations to be performed asynchronously and overlapped with computation, which should allow some of this overhead to be minimised in future versions of GeNN.
Because the Jetson TX2 is a system-on-chip in which the CPU and GPU cores share the same physical memory, no copying of data over the PCI Express bus is required and the overhead of the simulations running on this system are significantly lower than on any of the discrete GPUs.
In fact, when we simulated the microcircuit model at the smallest scale, figure~\ref{fig:microcircuit_performance_scaling} shows that the Jetson TX2 simulations actually ran \textit{faster} than those run on the GeForce 1050ti.

In sections~\ref{sec:method/genn},~\ref{sec:method/microcircuit}~and~\ref{sec:method/balanced_random} we discuss how GeNN uses the GPU to parallelise network initialisation and, in section~\ref{sec:results/performance/microcircuit}, we show how this benefits overall simulation run-times.
A similar approach could be used for analysis of simulation results, for example to reduce the \num{810E6} plastic synaptic weights in the balanced random network model to the histogram shown in figure~\ref{fig:mad_weights}.
Because of the low-level flexible nature of GeNN, this \textit{could} be implemented in a CUDA kernel provided by the user.
However, downloading the plastic weights in the balanced random network model from the GPU to the host computer only takes around \SI{300}{\milli\second} making it more practical to simply write these weights to disk and analyse them using one of the many CPU-based analysis tools available.

Unlike the simulations typically run on CPU-based systems, the GPU simulations presented in this paper use single rather than double-precision floating point and therefore have the potential for more numerical instability.
Additionally, the non-associative nature of floating point operations means that, if the results from a large number of parallel threads are summed together in a non-deterministic order, results can differ between runs due to rounding errors.
\citet{Villa2009} demonstrated that the result of summing \num{28000} double-precision floating point numbers across \num{16000} threads of a CRAY XMT system (which, in this context, has similar properties to a GPU) varied by up to \SI{24.64}{\percent}.
However, in this experiment, more numbers were summed than would occur when using any of the parallelisation schemes used in our simulations and it is unclear what absolute errors the reported relative errors correspond to. 
Furthermore, based on the analysis we presented in section~\ref{sec:results/correctness}, this potential source of error did not appear to affect our simulations suggesting that using single-precision floating point and summing inputs in a non-deterministic order has a minimal effect on the dynamics of the microcircuit model.

As we discussed in the introduction, the computational requirements of training Artificial Neural Networks~(ANNs) of ever-increasing size and complexity has been a major driver in the development of GPU hardware~\citep{Schmidhuber2015}.
%However, the modelling study performed by \citet{Esmaeilzadeh2012} indicates that this scaling of GPU performance will begin to slow as the `power wall' is reached.
These applications and the growing need to deploy ready-trained ANNs to perform inference in real time on low-power `edge computing' devices mean that available memory bandwidth is beginning to limit performance.
Although upcoming technologies such as third generation High Bandwidth Memory~(HBM3) are likely to offer increases in memory bandwidth in the future, alternative strategies are still going to be required to better utilise current GPUs for SNN simulation as well as to increase the size of models that can be simulated using embedded GPUs such as the Jetson TX2.
One solution, used successfully in ANN inference and training, has been to use lower precision \SI{16}{\bit} floating point and even fixed point integer representations for weights~\citep{Micikevicius2017}.
Using smaller data types not only saves memory and memory bandwidth but, on some newer GPU hardware including the Jetson TX2, each CUDA thread can perform four \SI{8}{\bit} or two \SI{16}{\bit} operations simultaneously -- significantly increasing peak performance.
While lower precision types are unlikely to provide enough numerical stability for storing neuron state variables, as discussed by \citet{VanAlbada2018}, the \SI{16}{\bit} fixed-point synaptic weights used by SpiNNaker provide sufficient accuracy for the microcircuit model described in section~\ref{sec:method/microcircuit}.
While GeNN does not currently support these lower-precision types, we plan on extending the algorithms described in section~\ref{sec:method} to support \SI{16}{\bit} floating point synaptic weights which should offer a significant performance improvement while not sacrificing the convenience of floating point programming.
%A more detailed treatment of the expected rounding errors in parallel simulations of SNNs on GPUs will be published elsewhere~\citep{Turner2019}. 

In this paper we have only considered single-GPU simulations of circuit-scale SNN models.
However, using supercomputer systems, models with up to a billion neurons can now be simulated~\citep{Jordan2018} and computational neuroscientists are beginning to use this capability to investigate the interactions between multiple circuit-scale models.
For example, \citet{Schmidt2015} developed a model of the Macaque visual cortex consisting of 32 cortical areas, each modelled as a customised version of the model described in section~\ref{sec:method/microcircuit}.
Even if such a model were implemented using half-precision floating point weights, a single GPU would not have enough memory to simulate it.
However, systems such as the NVIDIA DGX-2~\citep{NVIDIACorporation2018b} are now available which contain several Tesla V100 GPUs, connected through a crossbar with a \SI{900}{\giga\byte\per\second} bisection bandwidth.
While GeNN does not currently target such multi-GPU systems, because all of their GPUs are connected to a single host system and are all mapped into its memory space, they maintain many of the advantages of the single-GPU simulations discussed in this paper.
Beyond this scale, further parallelism could be achieved by using MPI to distribute GPU-accelerated simulations across multiple HPC nodes.
While using MPI would lead to simulation becoming communication bound, as is currently the case with CPU simulations, fewer more powerful GPU-equipped nodes should act to reduce this as well as reducing power usage. 
Additionally, communication overheads could be reduced by using NVIDIA's GPUDirect~\citep{NVIDIACorporation2018c} technology, allowing data to be transferred directly between remote GPUs via compatible network cards.

\subsection{Comparison to neuromorphic systems}
In section~\ref{sec:results/power} we showed that our GPU simulations of the microcircuit model required less energy than those run on SpiNNaker.
As \citeauthor{VanAlbada2018} discuss, this poor energy efficiency comes from slowing SpiNNaker simulations down by a factor of \num{20} and only simulating \num{80} neurons on each core.
However, because SpiNNaker is a software-programmable system, these limitations are not set in stone and \citet{Knight2016b} present some potential solutions to the underlying problems.
\citeauthor{Knight2016b} showed how models can be distributed more efficiently across a SpiNNaker machine and how Poisson background input could be directly injected into neurons to reduce the cost of incoming spike processing.
Additionally, other software techniques we present in the context of GeNN such as the bitmask connectivity format and the parallel connectivity initialisation would be potentially applicable to software-programmable systems such as SpiNNaker.
Therefore it seems possible that, purely through software improvements, SpiNNaker could become significantly more energy efficient -- perhaps closer to the \SI{0.11}{\micro\joule} per synaptic event measured by \citet{Sharp2012} on a less complex model.
Furthermore, by using more advanced power management techniques as well as reducing the manufacturing process size from \SI{130}{\nano\metre} to \SI{22}{\nano\meter}, the next generation SpiNNaker system aims to improve energy efficiency by a factor of \num{10}~\citep{Hoppner2017}.
Neuromorphic systems based on custom circuits rather than programmable CPUs still require much less energy.
Digital system such as Intel's Loihi~\citep{Davies2018} or IBM's TrueNorth~\citep{Merolla2014} only require around \SI{20}{\pico\joule} per-synaptic event and analogue systems such as Dynapse~\citep{Qiao2015} only require around \SI{100}{\femto\joule} per synaptic event.
However, beside from SpiNNaker, only the Intel Loihi supports the degree of connectivity required to implement the microcircuit model.

Simulating the balanced random network model described in section~\ref{sec:method/balanced_random} would be an even greater challenge for a neuromorphic system as, again, only SpiNNaker and Loihi would be able to support its high degree of connectivity.
Additionally, while Loihi has a relatively flexible microcode-based `learning engine', it does not directly support the operations required to calculate the $w_{ij}^{\mu}$ term in equation~\ref{eq:stdp_update_post_pair}.
While several relatively complex synaptic plasticity rules have previously been implemented on SpiNNaker~\citep{Knight2016,Mikaitis2018b}, these only required exponential decays and logarithms which could be implement using lookup tables, whereas, evaluating $w_{ij}^{\mu}$ would be likely to require a series expansion.
\citet{Moise2012} implemented several transcendental functions on SpiNNaker using series expansions and showed that they typically required in the order of \num{100} CPU cycles.
\citet{Knight2016b} analysed the performance of STDP processing on SpiNNaker and found that performing an additive weight update in response to a postsynaptic spikes took around \num{31} CPU cycles.
Therefore, adding \num{100} extra CPU cycles to this update would be likely to severely reduce the STDP processing performance of SpiNNaker to the point that it would be unable to efficiently simulate this model.
%Similarily the next generation BrainScaleS system will include vector-processors adjacent to each synapse array~\citep{Friedmann2016}
However, the next generation SpiNNaker system is likely to include bespoke accelerators to provide acceleration for $exp(x)$ and $ln(x)$~\citep{Mikaitis2018,Partzsch2017} which could be used to implement $w_{ij}^{\mu}$ as $\exp(\mu * \log(x))$.

\subsection{Neurorobotics}
\label{sec:discussion/neurobotics}
Neurorobotics involves the development of robots with controllers inspired by the brain, allowing neural function to be studied in an embodied context.
Neuro robots have been developed with controllers inspired by the mammalian Hippocampus~\citep{Krichmar2005} as well as the honey bee~\citep{Cope2016} and other insects~\citep{Blanchard2000}.
However, computational constraints have meant that these systems had to be operated either in simulation or their brain-inspired controllers had to be simulated externally to the robot.
While using an external controller removes any constraints on the power and weight of the controller, it also introduces latency, meaning robots must operate slower.
Additionally, communicating with an external controller typically means that a robot has to be `tethered' to a WiFi base station, restricting where it can operate.

The low power requirements and real-time performance of neuromorphic systems make them obvious candidates for building on-board neurorobotics controllers.
However, in order to interface with the robots' hardware and convert sensor data into spike trains, these systems typically need to be accompanied by a standard CPU.
For example, \citet{Kreiser2018} developed a path integration model on the Dynapse~\citep{Qiao2015} neuromorphic system which used a Parallela~\citep{Olofsson2015} board to interface with the robot and \citet{Hwu2017} developed a self-driving robot using a spiking convolutional neural network running on a TrueNorth NS1e development board which includes a Zynq SoC~\citep{XilinxInc2018}.
While both the Dynapse and TrueNorth systems have a negligible power consumption, the NS1e development board draws between \SIrange{2}{3}{\watt}~\citep{Sawada2016} and the Parallela \SI{5}{\watt}, somewhat out-weighing their theoretical advantages over embedded GPUs such as the Jetson TX2 which only draws a maximum of \SI{15}{\watt}.

Because SpiNNaker is built from programmable ARM cores, these can be repurposed for interfacing with robot hardware directly, for example using the interface board developed by \citet{Denk2013} which supports a variety of robots developed at the Technical University of Munich.
However, the 48 chip SpiNNaker board used on the robot developed by \citet{Conradt2015} is around $10\times$ larger than a Jetson TX2, restricting its use to large ground-based robots whereas the Jetson TX2 is small and light enough to be used on both ground and aerial robots.
%Additionally, figure~\ref{fig:microcircuit_performance_scaling} suggests that, due to the low cost of interactions between the CPU and GPU on the Jetson TX2, it 

\subsection{Interactive simulation}
As discussed in the introduction, one of the major uses of SNN simulations in computational neuroscience is for characterising and exploring the subset of models' parameter spaces left under-constrained by experimental data.

In common with many other application areas, computational neuroscience simulations are typically performed in a non-interactive `batch' mode in which a simulation is started (either on a remote HPC system or locally) and some time later results are returned.
The results of such simulations are then analysed offline to determine whether a particular combination of parameters has resulted in a successful emulation of a brain circuit.
However, it is difficult to determine what data will be required for this analysis ahead of time.
Recording too much data requires large amounts of disk space and potentially slows down both, simulation and analysis.
\textit{Computational steering}~\citep{Parker1997} could be one solution to this problem -- a technology that allows researchers to change the parameters of a running simulation as well as which state variables are being visualised.

With the development of large-scale models such as those discussed in the previous section, the need for approaches such as computation steering in computational neuroscience is becoming apparant.
\citet{Nowke2018} developed a computational steering system for visualising and steering NEST simulations.
However, when running this system across a CPU-based HPC system, \citeauthor{Nowke2018} found that its scalability was dictated by the amount of data that had to be transferred across the network at each simulation timestep.
The next generation of supercomputer systems are being designed specifically to address these issues~\citep{Lippert2014}.
However, as discussed in section~\ref{sec:discussion/neurobotics}, GPUs are a more natural fit for this type of tight interaction between visualisation and simulation as they exist within the host system's memory space, allowing data to be exchanged at the speed of the PCI express bus, rather than of an external network.
Additionally, because CUDA can interact directly with graphics APIs such as OpenGL, some simple visualisations could be rendered without any interaction with the host computers CPU at all.

\section*{Conflict of Interest Statement}
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}
JK and TN wrote the paper.
TN is the original developer of GeNN.
JK is currently the primary GeNN developer and was responsible for extending the code generation approach to the parallel initialisation of networks.
JK performed the experiments and the analysis of their results which are presented in this work.

\section*{Funding}
This work was funded by the EPSRC (Brains on Board project, grant number EP/P006094/1).

\section*{Acknowledgments}
We would like to thank Andrew Webb for his thoughts on efficient parallel connectivity generation.
We would also like to thank Sacha van Albada for providing the data from her NEST simulations and clarifying some parts of the accuracy analysis -- without these contributions section~\ref{sec:results/correctness} would not have been possible.

\section*{Data Availability Statement}
All models, data and analysis scripts used for this study can be found in \url{https://github.com/neworderofjamie/frontiers_genn_paper}.
% Please see the availability of data guidelines for more information, at https://www.frontiersin.org/about/author-guidelines#AvailabilityofData
%
\bibliographystyle{frontiersinSCNS_ENG_HUMS}
\bibliography{frontiers_genn}

%%% Make sure to upload the bib file along with the tex file and PDF
%%% Please see the test.bib file for some examples of references

\end{document}
